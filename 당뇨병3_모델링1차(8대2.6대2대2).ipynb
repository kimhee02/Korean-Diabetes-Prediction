{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65384b6a-97fc-4e83-833b-d55aba6e8b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "male_df = pd.read_csv(\"C:/Users/JEONGHEE/Desktop/당뇨병플젝/male.csv\")\n",
    "female_df = pd.read_csv(\"C:/Users/JEONGHEE/Desktop/당뇨병플젝/female.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1368a14-22e4-4beb-9663-844636c8eab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3763639d-b1d3-4732-a219-72d67307a059",
   "metadata": {},
   "source": [
    "# 남자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a7412e-770a-497b-8051-f94dd7e553f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 랜덤포레스트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35ef40c2-5b3c-4dc2-bc31-41c346937e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 언더샘플링 안한 상태 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ecdba9e-c498-442c-b097-89b47551843a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터 성능 평가:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      비당뇨(0)       0.96      1.00      0.98      6991\n",
      "       당뇨(1)       0.00      0.00      0.00       321\n",
      "\n",
      "    accuracy                           0.96      7312\n",
      "   macro avg       0.48      0.50      0.49      7312\n",
      "weighted avg       0.91      0.96      0.93      7312\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "drop_cols = ['DS1_ID', 'DS1_SEX', 'DS1_GLU0', 'DS1_HBA1C']\n",
    "\n",
    "# 입력(X) / 타겟(y) 정의\n",
    "X = male_df.drop(columns=drop_cols + ['target'])\n",
    "y = male_df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "print(\"테스트 데이터 성능 평가:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['비당뇨(0)', '당뇨(1)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "730974c8-4214-43e4-a62e-8d204479a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 언더샘플링만 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7611f9f6-a4d0-4cbb-99cf-8d78c7133511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터 성능 평가:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.68      0.67       322\n",
      "           1       0.67      0.65      0.66       321\n",
      "\n",
      "    accuracy                           0.67       643\n",
      "   macro avg       0.67      0.67      0.67       643\n",
      "weighted avg       0.67      0.67      0.67       643\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "\n",
    "drop_cols = ['DS1_ID', 'DS1_SEX', 'DS1_GLU0', 'DS1_HBA1C']\n",
    "\n",
    "# 입력(X) / 타겟(y) 정의\n",
    "X = male_df.drop(columns=drop_cols + ['target'])\n",
    "y = male_df['target']\n",
    "\n",
    "\n",
    "df_all = X.copy()\n",
    "df_all['target'] = y\n",
    "\n",
    "df_majority = df_all[df_all['target'] == 0]\n",
    "df_minority = df_all[df_all['target'] == 1]\n",
    "\n",
    "df_majority_downsampled = resample(\n",
    "    df_majority,\n",
    "    replace=False,\n",
    "    n_samples=len(df_minority),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df_balanced = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "X_bal = df_balanced.drop(columns=['target'])\n",
    "y_bal = df_balanced['target']\n",
    "\n",
    "# 학습용 80% / 테스트용 20%만 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_bal, y_bal, test_size=0.2, stratify=y_bal, random_state=42\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "#테스트 데이터 예측 및 성능 평가\n",
    "test_pred = rf_model.predict(X_test)\n",
    "print(\"테스트 데이터 성능 평가:\")\n",
    "print(classification_report(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f453a2b1-b586-40ce-94aa-585a4455d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 언더샘플링 + 하이퍼파라미터 + 임계값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36fb2a6f-d508-4ff5-96c6-918af24f48ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [4] 튜닝 적용 (max_depth=10, min_samples_leaf=3):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.68      0.70       322\n",
      "           1       0.69      0.74      0.71       321\n",
      "\n",
      "    accuracy                           0.71       643\n",
      "   macro avg       0.71      0.71      0.71       643\n",
      "weighted avg       0.71      0.71      0.71       643\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "\n",
    "drop_cols = ['DS1_ID', 'DS1_SEX', 'DS1_GLU0', 'DS1_HBA1C']\n",
    "\n",
    "X = male_df.drop(columns=drop_cols + ['target'])\n",
    "y = male_df['target']\n",
    "\n",
    "\n",
    "df_all = X.copy()\n",
    "df_all['target'] = y\n",
    "df_majority = df_all[df_all['target'] == 0]\n",
    "df_minority = df_all[df_all['target'] == 1]\n",
    "df_majority_down = resample(df_majority, replace=False, n_samples=len(df_minority), random_state=42)\n",
    "df_balanced = pd.concat([df_majority_down, df_minority])\n",
    "\n",
    "\n",
    "X_bal = df_balanced.drop(columns=['target'])\n",
    "y_bal = df_balanced['target']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bal, y_bal, test_size=0.2, stratify=y_bal, random_state=42)\n",
    "\n",
    "\n",
    "rf4 = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=300,\n",
    "    min_samples_leaf=60,\n",
    "    random_state=42\n",
    ")\n",
    "rf4.fit(X_train, y_train)\n",
    "pred4 = rf4.predict(X_test)\n",
    "print(\" [4] 튜닝 적용 (max_depth=10, min_samples_leaf=3):\")\n",
    "print(classification_report(y_test, pred4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79badbe6-f2dd-40f2-8243-ee62a0c41515",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 언더샘플링 + 하이퍼파라미터 + 임계값 + threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47eda7e9-8390-4c42-9d4e-c9d93fa246ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제거된 변수: DS1_WALK (VIF: 653212061.71)\n",
      "제거된 변수: DS1_WALKT (VIF: 2404511.08)\n",
      "제거된 변수: DS1_WAIST (VIF: 141.95)\n",
      "제거된 변수: DS1_BODYFAT (VIF: 34.30)\n",
      "\n",
      " Threshold별 성능 비교:\n",
      "    Threshold  Accuracy  Precision  Recall  F1 Score\n",
      "0        0.30     0.580      0.544   0.978     0.699\n",
      "1        0.31     0.586      0.549   0.969     0.700\n",
      "2        0.32     0.591      0.552   0.960     0.701\n",
      "3        0.33     0.605      0.561   0.960     0.708\n",
      "4        0.34     0.611      0.566   0.947     0.709\n",
      "5        0.35     0.624      0.576   0.931     0.712\n",
      "6        0.36     0.639      0.588   0.922     0.718\n",
      "7        0.37     0.645      0.594   0.916     0.721\n",
      "8        0.38     0.645      0.596   0.900     0.717\n",
      "9        0.39     0.644      0.597   0.879     0.711\n",
      "10       0.40     0.649      0.604   0.860     0.710\n",
      "11       0.41     0.658      0.613   0.850     0.713\n",
      "12       0.42     0.659      0.617   0.838     0.711\n",
      "13       0.43     0.652      0.614   0.816     0.701\n",
      "14       0.44     0.655      0.618   0.807     0.700\n",
      "15       0.45     0.655      0.620   0.798     0.698\n",
      "16       0.46     0.650      0.621   0.769     0.687\n",
      "17       0.47     0.659      0.635   0.748     0.687\n",
      "18       0.48     0.658      0.640   0.720     0.677\n",
      "19       0.49     0.653      0.642   0.688     0.665\n",
      "20       0.50     0.659      0.659   0.657     0.658\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------------------\n",
    "# 1. 변수 제거 + 스케일링\n",
    "drop_cols = ['DS1_ID', 'DS1_SEX', 'DS1_GLU0', 'DS1_HBA1C']\n",
    "X = male_df.drop(columns=drop_cols + ['target'])\n",
    "y = male_df['target']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# -------------------------------------------\n",
    "# 2. VIF 기반 변수 제거\n",
    "def calculate_vif(X, thresh=10.0):\n",
    "    variables = X.columns.tolist()\n",
    "    while True:\n",
    "        vif = [variance_inflation_factor(X[variables].values, i) for i in range(len(variables))]\n",
    "        max_vif = max(vif)\n",
    "        if max_vif > thresh:\n",
    "            max_index = vif.index(max_vif)\n",
    "            print(f\"제거된 변수: {variables[max_index]} (VIF: {max_vif:.2f})\")\n",
    "            variables.pop(max_index)\n",
    "        else:\n",
    "            break\n",
    "    return X[variables]\n",
    "\n",
    "X_vif_filtered = calculate_vif(X_scaled)\n",
    "\n",
    "# -------------------------------------------\n",
    "# 3. 언더샘플링\n",
    "df_all = X_vif_filtered.copy()\n",
    "df_all['target'] = y\n",
    "\n",
    "df_majority = df_all[df_all['target'] == 0]\n",
    "df_minority = df_all[df_all['target'] == 1]\n",
    "\n",
    "df_majority_downsampled = resample(\n",
    "    df_majority,\n",
    "    replace=False,\n",
    "    n_samples=len(df_minority),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df_balanced = pd.concat([df_majority_downsampled, df_minority])\n",
    "X_bal = df_balanced.drop(columns=['target'])\n",
    "y_bal = df_balanced['target']\n",
    "\n",
    "# -------------------------------------------\n",
    "# 4. 학습/테스트 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_bal, y_bal, test_size=0.2, stratify=y_bal, random_state=42\n",
    ")\n",
    "\n",
    "# -------------------------------------------\n",
    "# 5. 랜덤포레스트 학습\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# -------------------------------------------\n",
    "# 6. Threshold 조정 실험\n",
    "thresholds = np.arange(0.30, 0.51, 0.01)\n",
    "results = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_proba_rf >= threshold).astype(int)\n",
    "    results.append({\n",
    "        'Threshold': round(threshold, 2),\n",
    "        'Accuracy': round(accuracy_score(y_test, y_pred), 3),\n",
    "        'Precision': round(precision_score(y_test, y_pred), 3),\n",
    "        'Recall': round(recall_score(y_test, y_pred), 3),\n",
    "        'F1 Score': round(f1_score(y_test, y_pred), 3)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n Threshold별 성능 비교:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd8749c4-7770-4c97-9b54-6e5e2ee44aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "랜덤포레스트 자동 튜닝 결과 (언더샘플링 + StandardScaler):\n",
      "   Model_ID  n_estimators  max_depth  min_samples_leaf max_features  \\\n",
      "14    RF_15           300         10                50         sqrt   \n",
      "2      RF_3           200         10                50         sqrt   \n",
      "22    RF_23           300         50                50         sqrt   \n",
      "6      RF_7           200         30                50         sqrt   \n",
      "18    RF_19           300         30                50         sqrt   \n",
      "10    RF_11           200         50                50         sqrt   \n",
      "12    RF_13           300         10                10         sqrt   \n",
      "3      RF_4           200         10                50          0.5   \n",
      "20    RF_21           300         50                10         sqrt   \n",
      "7      RF_8           200         30                50          0.5   \n",
      "11    RF_12           200         50                50          0.5   \n",
      "16    RF_17           300         30                10         sqrt   \n",
      "19    RF_20           300         30                50          0.5   \n",
      "15    RF_16           300         10                50          0.5   \n",
      "23    RF_24           300         50                50          0.5   \n",
      "8      RF_9           200         50                10         sqrt   \n",
      "4      RF_5           200         30                10         sqrt   \n",
      "0      RF_1           200         10                10         sqrt   \n",
      "1      RF_2           200         10                10          0.5   \n",
      "17    RF_18           300         30                10          0.5   \n",
      "21    RF_22           300         50                10          0.5   \n",
      "13    RF_14           300         10                10          0.5   \n",
      "9     RF_10           200         50                10          0.5   \n",
      "5      RF_6           200         30                10          0.5   \n",
      "\n",
      "   class_weight  Accuracy  Precision  Recall     F1    AUC  AUC >= 0.8  \n",
      "14     balanced     0.709      0.696   0.741  0.718  0.767       False  \n",
      "2      balanced     0.706      0.696   0.729  0.712  0.767       False  \n",
      "22     balanced     0.709      0.696   0.741  0.718  0.767       False  \n",
      "6      balanced     0.711      0.699   0.738  0.718  0.767       False  \n",
      "18     balanced     0.709      0.696   0.741  0.718  0.767       False  \n",
      "10     balanced     0.711      0.699   0.738  0.718  0.767       False  \n",
      "12     balanced     0.694      0.685   0.717  0.700  0.758       False  \n",
      "3      balanced     0.709      0.691   0.754  0.721  0.757       False  \n",
      "20     balanced     0.703      0.692   0.729  0.710  0.757       False  \n",
      "7      balanced     0.708      0.689   0.754  0.720  0.757       False  \n",
      "11     balanced     0.708      0.689   0.754  0.720  0.757       False  \n",
      "16     balanced     0.703      0.692   0.729  0.710  0.757       False  \n",
      "19     balanced     0.700      0.683   0.745  0.712  0.756       False  \n",
      "15     balanced     0.698      0.682   0.741  0.710  0.756       False  \n",
      "23     balanced     0.700      0.683   0.745  0.712  0.756       False  \n",
      "8      balanced     0.701      0.694   0.720  0.706  0.756       False  \n",
      "4      balanced     0.701      0.694   0.720  0.706  0.756       False  \n",
      "0      balanced     0.684      0.674   0.713  0.693  0.754       False  \n",
      "1      balanced     0.689      0.681   0.710  0.695  0.748       False  \n",
      "17     balanced     0.683      0.673   0.710  0.691  0.746       False  \n",
      "21     balanced     0.683      0.673   0.710  0.691  0.746       False  \n",
      "13     balanced     0.680      0.671   0.704  0.687  0.745       False  \n",
      "9      balanced     0.689      0.676   0.723  0.699  0.744       False  \n",
      "5      balanced     0.689      0.676   0.723  0.699  0.744       False  \n",
      "\n",
      "최고 AUC 모델: RF_15 (AUC=0.767)\n",
      "    Threshold  Accuracy  Precision  Recall  F1 Score    AUC\n",
      "0        0.30     0.565      0.535   0.981     0.692  0.767\n",
      "1        0.31     0.577      0.542   0.981     0.698  0.767\n",
      "2        0.32     0.589      0.550   0.981     0.705  0.767\n",
      "3        0.33     0.596      0.554   0.978     0.707  0.767\n",
      "4        0.34     0.597      0.555   0.972     0.707  0.767\n",
      "5        0.35     0.607      0.562   0.966     0.710  0.767\n",
      "6        0.36     0.611      0.565   0.960     0.711  0.767\n",
      "7        0.37     0.622      0.573   0.953     0.716  0.767\n",
      "8        0.38     0.630      0.579   0.944     0.718  0.767\n",
      "9        0.39     0.642      0.590   0.931     0.722  0.767\n",
      "10       0.40     0.656      0.601   0.925     0.729  0.767\n",
      "11       0.41     0.664      0.609   0.916     0.731  0.767\n",
      "12       0.42     0.666      0.612   0.903     0.730  0.767\n",
      "13       0.43     0.672      0.618   0.897     0.732  0.767\n",
      "14       0.44     0.680      0.626   0.888     0.735  0.767\n",
      "15       0.45     0.684      0.634   0.869     0.733  0.767\n",
      "16       0.46     0.692      0.645   0.854     0.735  0.767\n",
      "17       0.47     0.692      0.651   0.826     0.728  0.767\n",
      "18       0.48     0.708      0.671   0.813     0.735  0.767\n",
      "19       0.49     0.715      0.689   0.785     0.734  0.767\n",
      "20       0.50     0.709      0.696   0.741     0.718  0.767\n",
      "\n",
      "Classification Report (threshold=0.5 기준):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.68      0.70       322\n",
      "           1       0.70      0.74      0.72       321\n",
      "\n",
      "    accuracy                           0.71       643\n",
      "   macro avg       0.71      0.71      0.71       643\n",
      "weighted avg       0.71      0.71      0.71       643\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, classification_report\n",
    ")\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "drop_cols = ['DS1_ID', 'DS1_SEX', 'DS1_GLU0', 'DS1_HBA1C',\n",
    "             'DS1_WALK', 'DS1_WALKT', 'DS1_WAIST', 'DS1_BODYFAT', 'DS1_MARRY_RE']\n",
    "X = male_df.drop(columns=drop_cols + ['target'])\n",
    "y = male_df['target']\n",
    "\n",
    "\n",
    "df_all = X.copy()\n",
    "df_all['target'] = y\n",
    "df_majority = df_all[df_all['target'] == 0]\n",
    "df_minority = df_all[df_all['target'] == 1]\n",
    "df_majority_down = resample(df_majority, replace=False, n_samples=len(df_minority), random_state=42)\n",
    "df_balanced = pd.concat([df_majority_down, df_minority])\n",
    "X_bal = df_balanced.drop(columns=['target'])\n",
    "y_bal = df_balanced['target']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_bal_scaled = pd.DataFrame(scaler.fit_transform(X_bal), columns=X_bal.columns) #-> 트리구조는 필요 없음 \n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X_bal_scaled, y_bal, test_size=0.2, stratify=y_bal, random_state=42\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=0.25, stratify=y_trainval, random_state=42\n",
    ")\n",
    "X_train_full = pd.concat([X_train, X_val])\n",
    "y_train_full = pd.concat([y_train, y_val])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 300],\n",
    "    'max_depth': [10, 30, 50],\n",
    "    'min_samples_leaf': [10, 50],\n",
    "    'max_features': ['sqrt', 0.5],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "param_combinations = list(product(\n",
    "    param_grid['n_estimators'],\n",
    "    param_grid['max_depth'],\n",
    "    param_grid['min_samples_leaf'],\n",
    "    param_grid['max_features'],\n",
    "    param_grid['class_weight']\n",
    "))\n",
    "\n",
    "\n",
    "results = []\n",
    "for i, (n_est, depth, leaf, feat, weight) in enumerate(param_combinations):\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_est,\n",
    "        max_depth=depth,\n",
    "        min_samples_leaf=leaf,\n",
    "        max_features=feat,\n",
    "        class_weight=weight,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train_full, y_train_full)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_proba >= 0.5).astype(int)\n",
    "\n",
    "    results.append({\n",
    "        'Model_ID': f\"RF_{i+1}\",\n",
    "        'n_estimators': n_est,\n",
    "        'max_depth': depth,\n",
    "        'min_samples_leaf': leaf,\n",
    "        'max_features': feat,\n",
    "        'class_weight': weight,\n",
    "        'Accuracy': round(accuracy_score(y_test, y_pred), 3),\n",
    "        'Precision': round(precision_score(y_test, y_pred), 3),\n",
    "        'Recall': round(recall_score(y_test, y_pred), 3),\n",
    "        'F1': round(f1_score(y_test, y_pred), 3),\n",
    "        'AUC': round(roc_auc_score(y_test, y_proba), 3)\n",
    "    })\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['AUC >= 0.8'] = results_df['AUC'] >= 0.8\n",
    "results_df = results_df.sort_values(by='AUC', ascending=False)\n",
    "\n",
    "print(\"\\n랜덤포레스트 자동 튜닝 결과 (언더샘플링 + StandardScaler):\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "best_row = results_df.iloc[0]\n",
    "print(f\"\\n최고 AUC 모델: {best_row['Model_ID']} (AUC={best_row['AUC']})\")\n",
    "\n",
    "best_model = RandomForestClassifier(\n",
    "    n_estimators=int(best_row['n_estimators']),\n",
    "    max_depth=int(best_row['max_depth']),\n",
    "    min_samples_leaf=int(best_row['min_samples_leaf']),\n",
    "    max_features=best_row['max_features'],\n",
    "    class_weight=best_row['class_weight'],\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "best_model.fit(X_train_full, y_train_full)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "thresholds = np.arange(0.30, 0.51, 0.01)\n",
    "thresh_results = []\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred = (y_proba >= t).astype(int)\n",
    "    thresh_results.append({\n",
    "        'Threshold': round(t, 2),\n",
    "        'Accuracy': round(accuracy_score(y_test, y_pred), 3),\n",
    "        'Precision': round(precision_score(y_test, y_pred), 3),\n",
    "        'Recall': round(recall_score(y_test, y_pred), 3),\n",
    "        'F1 Score': round(f1_score(y_test, y_pred), 3),\n",
    "        'AUC': round(roc_auc_score(y_test, y_proba), 3)\n",
    "    })\n",
    "\n",
    "thresh_df = pd.DataFrame(thresh_results)\n",
    "\n",
    "print(thresh_df)\n",
    "\n",
    "\n",
    "final_pred = (y_proba >= 0.5).astype(int)\n",
    "print(\"\\nClassification Report (threshold=0.5 기준):\")\n",
    "print(classification_report(y_test, final_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b1412-cc27-4b9a-a101-294d83c95ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b97869b9-94b2-463f-8f70-f329e64f6f0b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "524d501a-4c14-4db2-bf20-e2c54d1e7eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:17:05] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:17:05] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:17:05] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:17:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:17:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:17:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:17:07] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:17:07] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:17:07] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:17:08] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:17:09] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:17:09] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:17:10] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:17:10] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:17:10] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:17:11] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:17:11] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:17:12] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:17:12] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:17:13] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "상위 20개 XGBoost 튜닝 결과:\n",
      "   Model_ID  n_estimators  max_depth  learning_rate  subsample  \\\n",
      "4     XGB_5           100          3           0.10        0.8   \n",
      "5     XGB_6           100          3           0.10        0.8   \n",
      "8     XGB_9           100          5           0.01        0.8   \n",
      "2     XGB_3           100          3           0.01        1.0   \n",
      "6     XGB_7           100          3           0.10        1.0   \n",
      "11   XGB_12           100          5           0.01        1.0   \n",
      "7     XGB_8           100          3           0.10        1.0   \n",
      "9    XGB_10           100          5           0.01        0.8   \n",
      "1     XGB_2           100          3           0.01        0.8   \n",
      "0     XGB_1           100          3           0.01        0.8   \n",
      "10   XGB_11           100          5           0.01        1.0   \n",
      "3     XGB_4           100          3           0.01        1.0   \n",
      "16   XGB_17           100          7           0.01        0.8   \n",
      "15   XGB_16           100          5           0.10        1.0   \n",
      "13   XGB_14           100          5           0.10        0.8   \n",
      "14   XGB_15           100          5           0.10        1.0   \n",
      "17   XGB_18           100          7           0.01        0.8   \n",
      "18   XGB_19           100          7           0.01        1.0   \n",
      "12   XGB_13           100          5           0.10        0.8   \n",
      "19   XGB_20           100          7           0.01        1.0   \n",
      "\n",
      "    colsample_bytree  Test_Accuracy  Test_Precision  Test_Recall  Test_F1  \\\n",
      "4                0.8          0.681           0.674        0.701    0.687   \n",
      "5                1.0          0.684           0.679        0.698    0.688   \n",
      "8                0.8          0.692           0.682        0.717    0.699   \n",
      "2                0.8          0.689           0.678        0.717    0.697   \n",
      "6                0.8          0.686           0.676        0.713    0.694   \n",
      "11               1.0          0.687           0.686        0.688    0.687   \n",
      "7                1.0          0.673           0.673        0.673    0.673   \n",
      "9                1.0          0.692           0.681        0.720    0.700   \n",
      "1                1.0          0.687           0.678        0.713    0.695   \n",
      "0                0.8          0.691           0.683        0.710    0.696   \n",
      "10               0.8          0.687           0.683        0.698    0.690   \n",
      "3                1.0          0.686           0.682        0.695    0.688   \n",
      "16               0.8          0.673           0.670        0.682    0.676   \n",
      "15               1.0          0.670           0.667        0.679    0.673   \n",
      "13               1.0          0.669           0.668        0.670    0.669   \n",
      "14               0.8          0.666           0.665        0.667    0.666   \n",
      "17               1.0          0.681           0.671        0.710    0.690   \n",
      "18               0.8          0.669           0.670        0.664    0.667   \n",
      "12               0.8          0.663           0.662        0.660    0.661   \n",
      "19               1.0          0.666           0.668        0.657    0.662   \n",
      "\n",
      "    Test_AUC  AUC >= 0.8  \n",
      "4      0.756       False  \n",
      "5      0.754       False  \n",
      "8      0.754       False  \n",
      "2      0.751       False  \n",
      "6      0.751       False  \n",
      "11     0.750       False  \n",
      "7      0.750       False  \n",
      "9      0.750       False  \n",
      "1      0.749       False  \n",
      "0      0.748       False  \n",
      "10     0.748       False  \n",
      "3      0.747       False  \n",
      "16     0.745       False  \n",
      "15     0.743       False  \n",
      "13     0.741       False  \n",
      "14     0.739       False  \n",
      "17     0.738       False  \n",
      "18     0.733       False  \n",
      "12     0.729       False  \n",
      "19     0.727       False  \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.69      0.70       321\n",
      "           1       0.70      0.72      0.71       322\n",
      "\n",
      "    accuracy                           0.71       643\n",
      "   macro avg       0.71      0.71      0.71       643\n",
      "weighted avg       0.71      0.71      0.71       643\n",
      "\n",
      "AUC (Validation): 0.787\n",
      "    Threshold  Accuracy  Precision  Recall     F1    AUC\n",
      "0        0.40     0.698      0.644   0.885  0.745  0.756\n",
      "1        0.41     0.705      0.651   0.879  0.748  0.756\n",
      "2        0.42     0.700      0.651   0.860  0.741  0.756\n",
      "3        0.43     0.705      0.658   0.850  0.742  0.756\n",
      "4        0.44     0.712      0.667   0.844  0.746  0.756\n",
      "5        0.45     0.714      0.673   0.832  0.744  0.756\n",
      "6        0.46     0.703      0.670   0.798  0.728  0.756\n",
      "7        0.47     0.700      0.671   0.782  0.722  0.756\n",
      "8        0.48     0.697      0.674   0.760  0.714  0.756\n",
      "9        0.49     0.684      0.671   0.723  0.696  0.756\n",
      "10       0.50     0.681      0.674   0.701  0.687  0.756\n",
      "\n",
      "최종 Classification Report (Test, threshold = 0.5):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.66      0.68       322\n",
      "           1       0.67      0.70      0.69       321\n",
      "\n",
      "    accuracy                           0.68       643\n",
      "   macro avg       0.68      0.68      0.68       643\n",
      "weighted avg       0.68      0.68      0.68       643\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# 하이퍼파라미터 조합 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "param_combinations = list(product(\n",
    "    param_grid['n_estimators'],\n",
    "    param_grid['max_depth'],\n",
    "    param_grid['learning_rate'],\n",
    "    param_grid['subsample'],\n",
    "    param_grid['colsample_bytree']\n",
    "))[:20]\n",
    "\n",
    "results = []\n",
    "best_auc = 0\n",
    "best_model = None\n",
    "best_pred_proba = None\n",
    "\n",
    "for i, (n_est, depth, lr, subsample, colsample) in enumerate(param_combinations):\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=n_est,\n",
    "        max_depth=depth,\n",
    "        learning_rate=lr,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train_full, y_train_full)\n",
    "    y_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred_test = (y_proba_test >= 0.5).astype(int)\n",
    "    auc = roc_auc_score(y_test, y_proba_test)\n",
    "\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        best_model = model\n",
    "        best_pred_proba = y_proba_test\n",
    "\n",
    "    results.append({\n",
    "        'Model_ID': f\"XGB_{i+1}\",\n",
    "        'n_estimators': n_est,\n",
    "        'max_depth': depth,\n",
    "        'learning_rate': lr,\n",
    "        'subsample': subsample,\n",
    "        'colsample_bytree': colsample,\n",
    "        'Test_Accuracy': round(accuracy_score(y_test, y_pred_test), 3),\n",
    "        'Test_Precision': round(precision_score(y_test, y_pred_test), 3),\n",
    "        'Test_Recall': round(recall_score(y_test, y_pred_test), 3),\n",
    "        'Test_F1': round(f1_score(y_test, y_pred_test), 3),\n",
    "        'Test_AUC': round(auc, 3)\n",
    "    })\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['AUC >= 0.8'] = results_df['Test_AUC'] >= 0.8\n",
    "results_df = results_df.sort_values(by='Test_AUC', ascending=False)\n",
    "\n",
    "print(\"\\n상위 20개 XGBoost 튜닝 결과:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "y_val_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "y_val_pred = (y_val_proba >= 0.5).astype(int)\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "print(f\"AUC (Validation): {roc_auc_score(y_val, y_val_proba):.3f}\")\n",
    "\n",
    "\n",
    "thresholds = np.arange(0.40, 0.51, 0.01)\n",
    "best_results = []\n",
    "\n",
    "for t in thresholds:\n",
    "    pred = (best_pred_proba >= t).astype(int)\n",
    "    best_results.append({\n",
    "        'Threshold': round(t, 2),\n",
    "        'Accuracy': round(accuracy_score(y_test, pred), 3),\n",
    "        'Precision': round(precision_score(y_test, pred), 3),\n",
    "        'Recall': round(recall_score(y_test, pred), 3),\n",
    "        'F1': round(f1_score(y_test, pred), 3),\n",
    "        'AUC': round(best_auc, 3)\n",
    "    })\n",
    "\n",
    "best_thresh_df = pd.DataFrame(best_results)\n",
    "print(best_thresh_df)\n",
    "\n",
    "print(\"\\n최종 Classification Report (Test, threshold = 0.5):\")\n",
    "final_pred = (best_pred_proba >= 0.5).astype(int)\n",
    "print(classification_report(y_test, final_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e18a34b-89b3-4c35-a85c-b848bc62db99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16abeb6-190c-4c44-9f78-0f50974eb4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fe5e219-7ecf-4d73-bfc2-617a3fc6ca02",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 로지스틱회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc3ca7b6-c8a1-494d-bd85-5645a24143c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제거된 변수: DS1_WALK (VIF: 653212061.71)\n",
      "제거된 변수: DS1_WALKT (VIF: 2404511.08)\n",
      "제거된 변수: DS1_WAIST (VIF: 141.95)\n",
      "제거된 변수: DS1_BODYFAT (VIF: 34.30)\n",
      "\n",
      "최종 남은 변수:\n",
      "['DS1_AGE', 'DS1_HTN', 'DS1_FDM', 'DS1_EXER', 'DS1_WALKFQ', 'DS1_HIP', 'DS1_PULSE', 'DS1_SBP', 'DS1_DBP', 'DS1_BMI', 'DS1_PBF', 'DS1_MUSCLE', 'DS1_VISFAT', 'DS1_WHR', 'DS1_INCOME_RE', 'DS1_EDU_RE', 'DS1_MARRY_RE', 'total_exer', 'DS1_SMOKE_RE', 'DS1_DRINK_RE']\n",
      "언더샘플링 후 클래스 비율:\n",
      " target\n",
      "0    1607\n",
      "1    1607\n",
      "Name: count, dtype: int64\n",
      "훈련 데이터 크기: (1928, 20)\n",
      "검증 데이터 크기: (643, 20)\n",
      "테스트 데이터 크기: (643, 20)\n",
      "\n",
      "정확도(Accuracy): 0.647\n",
      "\n",
      "혼동 행렬:\n",
      " [[212 110]\n",
      " [117 204]]\n",
      "\n",
      "분류 리포트:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.66      0.65       322\n",
      "           1       0.65      0.64      0.64       321\n",
      "\n",
      "    accuracy                           0.65       643\n",
      "   macro avg       0.65      0.65      0.65       643\n",
      "weighted avg       0.65      0.65      0.65       643\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# 1. 독립변수(X), 종속변수(y) 설정\n",
    "X = male_df.drop(columns=['target', 'DS1_SEX', 'DS1_GLU0', 'DS1_HBA1C', 'DS1_ID'])\n",
    "y = male_df['target']\n",
    "\n",
    "# 2. 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# 3. VIF 기준 변수 제거 함수 정의\n",
    "def calculate_vif(X, thresh=10.0):\n",
    "    variables = X.columns.tolist()\n",
    "    dropped = []\n",
    "\n",
    "    while True:\n",
    "        vif = [variance_inflation_factor(X[variables].values, i) for i in range(len(variables))]\n",
    "        max_vif = max(vif)\n",
    "        if max_vif > thresh:\n",
    "            max_index = vif.index(max_vif)\n",
    "            print(f\"제거된 변수: {variables[max_index]} (VIF: {max_vif:.2f})\")\n",
    "            dropped.append(variables[max_index])\n",
    "            variables.pop(max_index)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    print(\"\\n최종 남은 변수:\")\n",
    "    print(variables)\n",
    "    return X[variables], dropped\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4. VIF 기반 변수 제거 적용\n",
    "X_vif_filtered, dropped_vars = calculate_vif(X_scaled)\n",
    "\n",
    "# 5. 언더샘플링\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled_vif, y_resampled_vif = rus.fit_resample(X_vif_filtered, y)\n",
    "print(\"언더샘플링 후 클래스 비율:\\n\", y_resampled_vif.value_counts())\n",
    "\n",
    "# # 6. 훈련/테스트 분리\n",
    "# X_train_vif, X_test_vif, y_train_vif, y_test_vif = train_test_split(\n",
    "#     X_resampled_vif, y_resampled_vif, test_size=0.2, random_state=42, stratify=y_resampled_vif\n",
    "# )\n",
    "# print(\"훈련 데이터 크기:\", X_train_vif.shape)\n",
    "# print(\"테스트 데이터 크기:\", X_test_vif.shape)\n",
    "\n",
    "\n",
    "# 6. 훈련/검증/테스트 분리 (6:2:2 비율)\n",
    "# 전체에서 60% train, 40% temp (val + test)\n",
    "X_train_vif, X_temp_vif, y_train_vif, y_temp_vif = train_test_split(\n",
    "    X_resampled_vif, y_resampled_vif, test_size=0.4, random_state=42, stratify=y_resampled_vif\n",
    ")\n",
    "\n",
    "# temp에서 다시 50% val, 50% test → 각각 전체의 20%\n",
    "X_val_vif, X_test_vif, y_val_vif, y_test_vif = train_test_split(\n",
    "    X_temp_vif, y_temp_vif, test_size=0.5, random_state=42, stratify=y_temp_vif\n",
    ")\n",
    "\n",
    "print(\"훈련 데이터 크기:\", X_train_vif.shape)\n",
    "print(\"검증 데이터 크기:\", X_val_vif.shape)\n",
    "print(\"테스트 데이터 크기:\", X_test_vif.shape)\n",
    "\n",
    "# 7. 로지스틱 회귀 모델 학습 및 예측\n",
    "model_vif = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_vif.fit(X_train_vif, y_train_vif)\n",
    "y_pred_vif = model_vif.predict(X_test_vif)\n",
    "\n",
    "# 8. 성능 평가\n",
    "accuracy = accuracy_score(y_test_vif, y_pred_vif)\n",
    "cm = confusion_matrix(y_test_vif, y_pred_vif)\n",
    "report = classification_report(y_test_vif, y_pred_vif)\n",
    "\n",
    "print(f\"\\n정확도(Accuracy): {accuracy:.3f}\")\n",
    "print(\"\\n혼동 행렬:\\n\", cm)\n",
    "print(\"\\n분류 리포트:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46ea35e1-5cad-4513-906a-289215c428aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각 Fold 정확도: [0.62830482 0.66096423 0.65474339 0.64230171 0.62928349]\n",
      "평균 정확도: 0.6431195283014297\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "model_vif = LogisticRegression(max_iter=1000, random_state=42)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scores = cross_val_score(model_vif, X_resampled_vif, y_resampled_vif, cv=cv, scoring='accuracy')\n",
    "print(\"각 Fold 정확도:\", scores)\n",
    "print(\"평균 정확도:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab57a99b-f654-46f8-be41-41d01a2fca75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold별 성능 비교:\n",
      "    Threshold  Accuracy  Precision  Recall  F1 Score\n",
      "0        0.30     0.619      0.570   0.963     0.716\n",
      "1        0.31     0.628      0.577   0.953     0.719\n",
      "2        0.32     0.627      0.578   0.938     0.715\n",
      "3        0.33     0.636      0.584   0.938     0.720\n",
      "4        0.34     0.645      0.592   0.935     0.725\n",
      "5        0.35     0.650      0.597   0.919     0.724\n",
      "6        0.36     0.656      0.604   0.907     0.725\n",
      "7        0.37     0.659      0.609   0.888     0.722\n",
      "8        0.38     0.663      0.614   0.875     0.721\n",
      "9        0.39     0.667      0.620   0.860     0.721\n",
      "10       0.40     0.670      0.625   0.847     0.720\n",
      "11       0.41     0.678      0.635   0.835     0.721\n",
      "12       0.42     0.672      0.633   0.816     0.713\n",
      "13       0.43     0.663      0.629   0.791     0.701\n",
      "14       0.44     0.669      0.640   0.769     0.699\n",
      "15       0.45     0.670      0.644   0.760     0.697\n",
      "16       0.46     0.677      0.657   0.738     0.695\n",
      "17       0.47     0.669      0.653   0.717     0.684\n",
      "18       0.48     0.659      0.650   0.688     0.669\n",
      "19       0.49     0.652      0.649   0.657     0.653\n",
      "20       0.50     0.647      0.650   0.636     0.643\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# 1. 모델 학습\n",
    "model = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "model.fit(X_train_vif, y_train_vif)\n",
    "\n",
    "# 2. 확률 예측\n",
    "y_proba = model.predict_proba(X_test_vif)[:, 1]\n",
    "\n",
    "# 3. threshold 리스트 (0.3 ~ 0.5, 0.01 간격)\n",
    "thresholds = np.arange(0.30, 0.51, 0.01)\n",
    "\n",
    "# 4. 결과 저장\n",
    "results = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    acc = accuracy_score(y_test_vif, y_pred)\n",
    "    prec = precision_score(y_test_vif, y_pred)\n",
    "    rec = recall_score(y_test_vif, y_pred)\n",
    "    f1 = f1_score(y_test_vif, y_pred)\n",
    "    results.append({\n",
    "        'Threshold': round(threshold, 2),\n",
    "        'Accuracy': round(acc, 3),\n",
    "        'Precision': round(prec, 3),\n",
    "        'Recall': round(rec, 3),\n",
    "        'F1 Score': round(f1, 3)\n",
    "    })\n",
    "\n",
    "# 5. 결과 보기\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nThreshold별 성능 비교:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eccf5ba-f5e7-41f4-9823-c21b887751a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6590ab-1fa1-4192-95fb-5eee63616701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7825d8a3-c44e-4620-a6cc-c7a5588f7f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efdd1dde-9fb2-4522-bce6-9755c144b04c",
   "metadata": {},
   "source": [
    "# 여자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec377a0-c99e-4414-9a50-807bcc5b6789",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 랜덤포레스트 : 6:2:2 + 언더샘플링 + vif + 하이퍼파라미터 + threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36d1b2d3-efa8-472d-8283-4a2da915ae6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "상위 20개 랜덤포레스트 튜닝 결과 (여성):\n",
      "   Model_ID  n_estimators  max_depth  min_samples_leaf max_features  \\\n",
      "9   RF_F_10           200         10                10          0.5   \n",
      "11  RF_F_12           200         10                20          0.5   \n",
      "10  RF_F_11           200         10                20         sqrt   \n",
      "7    RF_F_8           200         10                 5          0.5   \n",
      "17  RF_F_18           200         15                20          0.5   \n",
      "15  RF_F_16           200         15                10          0.5   \n",
      "13  RF_F_14           200         15                 5          0.5   \n",
      "19  RF_F_20           200         20                 5          0.5   \n",
      "16  RF_F_17           200         15                20         sqrt   \n",
      "8    RF_F_9           200         10                10         sqrt   \n",
      "6    RF_F_7           200         10                 5         sqrt   \n",
      "12  RF_F_13           200         15                 5         sqrt   \n",
      "14  RF_F_15           200         15                10         sqrt   \n",
      "18  RF_F_19           200         20                 5         sqrt   \n",
      "1    RF_F_2           200          5                 5          0.5   \n",
      "5    RF_F_6           200          5                20          0.5   \n",
      "3    RF_F_4           200          5                10          0.5   \n",
      "4    RF_F_5           200          5                20         sqrt   \n",
      "0    RF_F_1           200          5                 5         sqrt   \n",
      "2    RF_F_3           200          5                10         sqrt   \n",
      "\n",
      "   class_weight  Test_Accuracy  Test_Precision  Test_Recall  Test_F1  \\\n",
      "9      balanced          0.669           0.649        0.734    0.689   \n",
      "11     balanced          0.668           0.649        0.728    0.686   \n",
      "10     balanced          0.669           0.648        0.740    0.691   \n",
      "7      balanced          0.663           0.645        0.722    0.682   \n",
      "17     balanced          0.663           0.644        0.728    0.683   \n",
      "15     balanced          0.669           0.651        0.728    0.687   \n",
      "13     balanced          0.669           0.653        0.719    0.685   \n",
      "19     balanced          0.668           0.652        0.716    0.683   \n",
      "16     balanced          0.669           0.648        0.737    0.690   \n",
      "8      balanced          0.671           0.651        0.734    0.690   \n",
      "6      balanced          0.672           0.651        0.740    0.693   \n",
      "12     balanced          0.671           0.652        0.728    0.688   \n",
      "14     balanced          0.669           0.648        0.737    0.690   \n",
      "18     balanced          0.665           0.647        0.722    0.683   \n",
      "1      balanced          0.660           0.640        0.731    0.682   \n",
      "5      balanced          0.666           0.647        0.728    0.685   \n",
      "3      balanced          0.656           0.637        0.722    0.677   \n",
      "4      balanced          0.657           0.639        0.719    0.677   \n",
      "0      balanced          0.657           0.638        0.725    0.679   \n",
      "2      balanced          0.657           0.638        0.725    0.679   \n",
      "\n",
      "    Test_AUC  AUC >= 0.8  \n",
      "9      0.734       False  \n",
      "11     0.733       False  \n",
      "10     0.732       False  \n",
      "7      0.732       False  \n",
      "17     0.732       False  \n",
      "15     0.732       False  \n",
      "13     0.732       False  \n",
      "19     0.732       False  \n",
      "16     0.731       False  \n",
      "8      0.730       False  \n",
      "6      0.730       False  \n",
      "12     0.729       False  \n",
      "14     0.729       False  \n",
      "18     0.728       False  \n",
      "1      0.725       False  \n",
      "5      0.725       False  \n",
      "3      0.725       False  \n",
      "4      0.724       False  \n",
      "0      0.724       False  \n",
      "2      0.723       False  \n",
      "\n",
      "최고 AUC 모델의 Validation Set 성능:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.76      0.80       335\n",
      "           1       0.78      0.87      0.82       336\n",
      "\n",
      "    accuracy                           0.81       671\n",
      "   macro avg       0.82      0.81      0.81       671\n",
      "weighted avg       0.82      0.81      0.81       671\n",
      "\n",
      "AUC (Validation): 0.909\n",
      "\n",
      "Test 기준 Threshold 실험:\n",
      "    Threshold  Accuracy  Precision  Recall     F1    AUC\n",
      "0        0.40     0.669      0.626   0.836  0.716  0.734\n",
      "1        0.41     0.672      0.630   0.830  0.716  0.734\n",
      "2        0.42     0.672      0.633   0.818  0.714  0.734\n",
      "3        0.43     0.666      0.630   0.803  0.706  0.734\n",
      "4        0.44     0.674      0.637   0.803  0.711  0.734\n",
      "5        0.45     0.677      0.643   0.794  0.710  0.734\n",
      "6        0.46     0.677      0.645   0.782  0.707  0.734\n",
      "7        0.47     0.678      0.650   0.770  0.705  0.734\n",
      "8        0.48     0.677      0.651   0.761  0.702  0.734\n",
      "9        0.49     0.669      0.648   0.740  0.691  0.734\n",
      "10       0.50     0.669      0.649   0.734  0.689  0.734\n",
      "\n",
      "최종 Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.60      0.65       336\n",
      "           1       0.65      0.73      0.69       335\n",
      "\n",
      "    accuracy                           0.67       671\n",
      "   macro avg       0.67      0.67      0.67       671\n",
      "weighted avg       0.67      0.67      0.67       671\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, classification_report\n",
    ")\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import product\n",
    "\n",
    "# ✅ 1. 데이터 불러오기\n",
    "female_df = pd.read_csv(\"C:/Users/JEONGHEE/Desktop/당뇨병플젝/female.csv\")\n",
    "\n",
    "# ✅ 2. 제거할 변수 정의 후 제거\n",
    "remove_cols = ['DS1_ID', 'DS1_SEX', 'DS1_GLU0', 'DS1_HBA1C',\n",
    "               'DS1_WALK', 'DS1_WALKT', 'DS1_WAIST', 'DS1_BODYFAT', 'DS1_MARRY_RE']\n",
    "X = female_df.drop(columns=remove_cols + ['target'])\n",
    "y = female_df['target']\n",
    "\n",
    "# ✅ 3. 언더샘플링\n",
    "df_all = X.copy()\n",
    "df_all['target'] = y\n",
    "df_majority = df_all[df_all['target'] == 0]\n",
    "df_minority = df_all[df_all['target'] == 1]\n",
    "df_majority_down = resample(df_majority, replace=False, n_samples=len(df_minority), random_state=42)\n",
    "df_balanced = pd.concat([df_majority_down, df_minority])\n",
    "X_bal = df_balanced.drop(columns=['target'])\n",
    "y_bal = df_balanced['target']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_bal_scaled = pd.DataFrame(scaler.fit_transform(X_bal), columns=X_bal.columns)\n",
    "\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X_bal_scaled, y_bal, test_size=0.2, stratify=y_bal, random_state=42\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=0.25, stratify=y_trainval, random_state=42\n",
    ")\n",
    "X_train_full = pd.concat([X_train, X_val])\n",
    "y_train_full = pd.concat([y_train, y_val])\n",
    "\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 300],\n",
    "    'max_depth': [5, 10, 15, 20, 30],\n",
    "    'min_samples_leaf': [5, 10, 20],\n",
    "    'max_features': ['sqrt', 0.5],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "param_combinations = list(product(\n",
    "    param_grid['n_estimators'],\n",
    "    param_grid['max_depth'],\n",
    "    param_grid['min_samples_leaf'],\n",
    "    param_grid['max_features'],\n",
    "    param_grid['class_weight']\n",
    "))[:20]\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "best_auc = 0\n",
    "best_model = None\n",
    "best_pred_proba = None\n",
    "\n",
    "for i, (n_est, depth, leaf, feat, weight) in enumerate(param_combinations):\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_est,\n",
    "        max_depth=depth,\n",
    "        min_samples_leaf=leaf,\n",
    "        max_features=feat,\n",
    "        class_weight=weight,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train_full, y_train_full)\n",
    "\n",
    "    y_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred_test = (y_proba_test >= 0.5).astype(int)\n",
    "    auc = roc_auc_score(y_test, y_proba_test)\n",
    "\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        best_model = model\n",
    "        best_pred_proba = y_proba_test\n",
    "\n",
    "    results.append({\n",
    "        'Model_ID': f\"RF_F_{i+1}\",\n",
    "        'n_estimators': n_est,\n",
    "        'max_depth': depth,\n",
    "        'min_samples_leaf': leaf,\n",
    "        'max_features': feat,\n",
    "        'class_weight': weight,\n",
    "        'Test_Accuracy': round(accuracy_score(y_test, y_pred_test), 3),\n",
    "        'Test_Precision': round(precision_score(y_test, y_pred_test), 3),\n",
    "        'Test_Recall': round(recall_score(y_test, y_pred_test), 3),\n",
    "        'Test_F1': round(f1_score(y_test, y_pred_test), 3),\n",
    "        'Test_AUC': round(auc, 3)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['AUC >= 0.8'] = results_df['Test_AUC'] >= 0.8\n",
    "results_df = results_df.sort_values(by='Test_AUC', ascending=False)\n",
    "\n",
    "print(\"\\n상위 20개 랜덤포레스트 튜닝 결과 (여성):\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"\\n최고 AUC 모델의 Validation Set 성능:\")\n",
    "y_val_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "y_val_pred = (y_val_proba >= 0.5).astype(int)\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "print(f\"AUC (Validation): {roc_auc_score(y_val, y_val_proba):.3f}\")\n",
    "\n",
    "\n",
    "print(\"\\nTest 기준 Threshold 실험:\")\n",
    "thresholds = np.arange(0.40, 0.51, 0.01)\n",
    "best_results = []\n",
    "\n",
    "for t in thresholds:\n",
    "    pred = (best_pred_proba >= t).astype(int)\n",
    "    best_results.append({\n",
    "        'Threshold': round(t, 2),\n",
    "        'Accuracy': round(accuracy_score(y_test, pred), 3),\n",
    "        'Precision': round(precision_score(y_test, pred), 3),\n",
    "        'Recall': round(recall_score(y_test, pred), 3),\n",
    "        'F1': round(f1_score(y_test, pred), 3),\n",
    "        'AUC': round(best_auc, 3)\n",
    "    })\n",
    "\n",
    "thresh_df = pd.DataFrame(best_results)\n",
    "print(thresh_df)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n최종 Classification Report (Test):\")\n",
    "final_pred = (best_pred_proba >= 0.5).astype(int)\n",
    "print(classification_report(y_test, final_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f5726-f4f2-45e4-9301-bd817079361a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8949d08d-2d7b-477e-b19d-4176e29ecc1a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## xgboost : 6:2:2 + 언더샘플링 + vif + 하이퍼파라미터 + threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc1c4b2b-1b4a-4e3e-994e-dc28ec96a3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:24:12] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:24:13] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:24:13] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:24:13] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:24:13] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:24:14] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:24:14] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:24:14] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:24:14] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:24:15] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:24:15] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:24:15] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:24:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:24:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:24:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:24:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:24:18] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:24:18] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:24:19] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\JEONGHEE\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:24:19] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "상위 20개 XGBoost 튜닝 결과 (여성):\n",
      "    Model_ID  n_estimators  max_depth  learning_rate  subsample  \\\n",
      "5    XGB_F_6           100          3           0.10        0.8   \n",
      "12  XGB_F_13           100          5           0.10        0.8   \n",
      "7    XGB_F_8           100          3           0.10        1.0   \n",
      "4    XGB_F_5           100          3           0.10        0.8   \n",
      "6    XGB_F_7           100          3           0.10        1.0   \n",
      "14  XGB_F_15           100          5           0.10        1.0   \n",
      "17  XGB_F_18           100          7           0.01        0.8   \n",
      "16  XGB_F_17           100          7           0.01        0.8   \n",
      "15  XGB_F_16           100          5           0.10        1.0   \n",
      "8    XGB_F_9           100          5           0.01        0.8   \n",
      "9   XGB_F_10           100          5           0.01        0.8   \n",
      "13  XGB_F_14           100          5           0.10        0.8   \n",
      "10  XGB_F_11           100          5           0.01        1.0   \n",
      "18  XGB_F_19           100          7           0.01        1.0   \n",
      "11  XGB_F_12           100          5           0.01        1.0   \n",
      "19  XGB_F_20           100          7           0.01        1.0   \n",
      "1    XGB_F_2           100          3           0.01        0.8   \n",
      "2    XGB_F_3           100          3           0.01        1.0   \n",
      "3    XGB_F_4           100          3           0.01        1.0   \n",
      "0    XGB_F_1           100          3           0.01        0.8   \n",
      "\n",
      "    colsample_bytree  Test_Accuracy  Test_Precision  Test_Recall  Test_F1  \\\n",
      "5                1.0          0.675           0.655        0.737    0.694   \n",
      "12               0.8          0.680           0.666        0.719    0.692   \n",
      "7                1.0          0.666           0.644        0.740    0.689   \n",
      "4                0.8          0.668           0.650        0.725    0.685   \n",
      "6                0.8          0.677           0.655        0.743    0.697   \n",
      "14               0.8          0.683           0.663        0.740    0.700   \n",
      "17               1.0          0.678           0.664        0.719    0.691   \n",
      "16               0.8          0.666           0.649        0.722    0.684   \n",
      "15               1.0          0.662           0.647        0.710    0.677   \n",
      "8                0.8          0.663           0.645        0.725    0.683   \n",
      "9                1.0          0.660           0.647        0.704    0.674   \n",
      "13               1.0          0.665           0.651        0.707    0.678   \n",
      "10               0.8          0.668           0.653        0.713    0.682   \n",
      "18               0.8          0.675           0.661        0.716    0.688   \n",
      "11               1.0          0.663           0.651        0.701    0.675   \n",
      "19               1.0          0.665           0.654        0.696    0.674   \n",
      "1                1.0          0.656           0.637        0.722    0.677   \n",
      "2                0.8          0.654           0.634        0.728    0.678   \n",
      "3                1.0          0.656           0.638        0.719    0.676   \n",
      "0                0.8          0.650           0.631        0.719    0.672   \n",
      "\n",
      "    Test_AUC  AUC >= 0.8  \n",
      "5      0.744       False  \n",
      "12     0.741       False  \n",
      "7      0.739       False  \n",
      "4      0.737       False  \n",
      "6      0.737       False  \n",
      "14     0.737       False  \n",
      "17     0.734       False  \n",
      "16     0.733       False  \n",
      "15     0.731       False  \n",
      "8      0.731       False  \n",
      "9      0.731       False  \n",
      "13     0.728       False  \n",
      "10     0.727       False  \n",
      "18     0.727       False  \n",
      "11     0.725       False  \n",
      "19     0.720       False  \n",
      "1      0.719       False  \n",
      "2      0.719       False  \n",
      "3      0.718       False  \n",
      "0      0.716       False  \n",
      "\n",
      "최고 AUC 모델의 Validation Set 성능:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.74      0.77       335\n",
      "           1       0.76      0.81      0.79       336\n",
      "\n",
      "    accuracy                           0.78       671\n",
      "   macro avg       0.78      0.78      0.78       671\n",
      "weighted avg       0.78      0.78      0.78       671\n",
      "\n",
      "AUC (Validation): 0.855\n",
      "\n",
      " Test 기준 Threshold 실험:\n",
      "    Threshold  Accuracy  Precision  Recall     F1    AUC\n",
      "0        0.40     0.675      0.632   0.836  0.720  0.744\n",
      "1        0.41     0.671      0.632   0.815  0.712  0.744\n",
      "2        0.42     0.674      0.636   0.809  0.712  0.744\n",
      "3        0.43     0.669      0.634   0.800  0.707  0.744\n",
      "4        0.44     0.669      0.635   0.791  0.705  0.744\n",
      "5        0.45     0.672      0.640   0.785  0.705  0.744\n",
      "6        0.46     0.678      0.647   0.782  0.708  0.744\n",
      "7        0.47     0.681      0.652   0.776  0.708  0.744\n",
      "8        0.48     0.677      0.651   0.761  0.702  0.744\n",
      "9        0.49     0.675      0.651   0.752  0.698  0.744\n",
      "10       0.50     0.675      0.655   0.737  0.694  0.744\n",
      "\n",
      "최종 Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.61      0.65       336\n",
      "           1       0.66      0.74      0.69       335\n",
      "\n",
      "    accuracy                           0.68       671\n",
      "   macro avg       0.68      0.68      0.67       671\n",
      "weighted avg       0.68      0.68      0.67       671\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, classification_report\n",
    ")\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import product\n",
    "female_df = pd.read_csv(\"C:/Users/JEONGHEE/Desktop/당뇨병플젝/female.csv\")\n",
    "\n",
    "\n",
    "\n",
    "remove_cols = ['DS1_ID', 'DS1_SEX', 'DS1_GLU0', 'DS1_HBA1C',\n",
    "               'DS1_WALK', 'DS1_WALKT', 'DS1_WAIST', 'DS1_BODYFAT', 'DS1_MARRY_RE']\n",
    "X = female_df.drop(columns=remove_cols + ['target'])\n",
    "y = female_df['target']\n",
    "\n",
    "\n",
    "\n",
    "X['target'] = y\n",
    "majority = X[X['target'] == 0]\n",
    "minority = X[X['target'] == 1]\n",
    "majority_down = resample(majority, replace=False, n_samples=len(minority), random_state=42)\n",
    "X_balanced = pd.concat([majority_down, minority])\n",
    "X_bal = X_balanced.drop(columns=['target'])\n",
    "y_bal = X_balanced['target']\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_bal_scaled = pd.DataFrame(scaler.fit_transform(X_bal), columns=X_bal.columns) #-> 로지스틱 회귀는 스케일링 진행\n",
    "\n",
    "\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X_bal_scaled, y_bal, test_size=0.2, stratify=y_bal, random_state=42\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=0.25, stratify=y_trainval, random_state=42\n",
    ")\n",
    "X_train_full = pd.concat([X_train, X_val])\n",
    "y_train_full = pd.concat([y_train, y_val])\n",
    "\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "param_combinations = list(product(\n",
    "    param_grid['n_estimators'],\n",
    "    param_grid['max_depth'],\n",
    "    param_grid['learning_rate'],\n",
    "    param_grid['subsample'],\n",
    "    param_grid['colsample_bytree']\n",
    "))[:20]\n",
    "\n",
    "results = []\n",
    "best_auc = 0\n",
    "best_model = None\n",
    "best_pred_proba = None\n",
    "\n",
    "for i, (n_est, depth, lr, subsample, colsample) in enumerate(param_combinations):\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=n_est,\n",
    "        max_depth=depth,\n",
    "        learning_rate=lr,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train_full, y_train_full)\n",
    "    y_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred_test = (y_proba_test >= 0.5).astype(int)\n",
    "    auc = roc_auc_score(y_test, y_proba_test)\n",
    "\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        best_model = model\n",
    "        best_pred_proba = y_proba_test\n",
    "\n",
    "    results.append({\n",
    "        'Model_ID': f\"XGB_F_{i+1}\",\n",
    "        'n_estimators': n_est,\n",
    "        'max_depth': depth,\n",
    "        'learning_rate': lr,\n",
    "        'subsample': subsample,\n",
    "        'colsample_bytree': colsample,\n",
    "        'Test_Accuracy': round(accuracy_score(y_test, y_pred_test), 3),\n",
    "        'Test_Precision': round(precision_score(y_test, y_pred_test), 3),\n",
    "        'Test_Recall': round(recall_score(y_test, y_pred_test), 3),\n",
    "        'Test_F1': round(f1_score(y_test, y_pred_test), 3),\n",
    "        'Test_AUC': round(auc, 3)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['AUC >= 0.8'] = results_df['Test_AUC'] >= 0.8\n",
    "results_df = results_df.sort_values(by='Test_AUC', ascending=False)\n",
    "\n",
    "print(\"\\n상위 20개 XGBoost 튜닝 결과 (여성):\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"\\n최고 AUC 모델의 Validation Set 성능:\")\n",
    "y_val_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "y_val_pred = (y_val_proba >= 0.5).astype(int)\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "print(f\"AUC (Validation): {roc_auc_score(y_val, y_val_proba):.3f}\")\n",
    "\n",
    "\n",
    "print(\"\\n Test 기준 Threshold 실험:\")\n",
    "thresholds = np.arange(0.40, 0.51, 0.01)\n",
    "best_results = []\n",
    "for t in thresholds:\n",
    "    pred = (best_pred_proba >= t).astype(int)\n",
    "    best_results.append({\n",
    "        'Threshold': round(t, 2),\n",
    "        'Accuracy': round(accuracy_score(y_test, pred), 3),\n",
    "        'Precision': round(precision_score(y_test, pred), 3),\n",
    "        'Recall': round(recall_score(y_test, pred), 3),\n",
    "        'F1': round(f1_score(y_test, pred), 3),\n",
    "        'AUC': round(best_auc, 3)\n",
    "    })\n",
    "thresh_df = pd.DataFrame(best_results)\n",
    "print(thresh_df)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n최종 Classification Report (Test):\")\n",
    "final_pred = (best_pred_proba >= 0.5).astype(int)\n",
    "print(classification_report(y_test, final_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d63d7c-934b-4659-9ef8-b6a93383c987",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
